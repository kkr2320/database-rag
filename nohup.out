CUDA extension not installed.
CUDA extension not installed.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/ec2-user/database-rag/quantize_base_model_gptq.py:26: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  input_ids = tokenized_data.input_ids[:, i:j]
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.07it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.05it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.67it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.41it/s]
INFO - Start quantizing layer 1/32
2024-06-23 17:03:48 INFO [auto_gptq.modeling._base] Start quantizing layer 1/32
INFO - Quantizing self_attn.k_proj in layer 1/32...
2024-06-23 17:03:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 1/32...
2024-06-23 17:04:00 INFO [auto_gptq.quantization.gptq] duration: 2.094298839569092
2024-06-23 17:04:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.v_proj in layer 1/32...
2024-06-23 17:04:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 1/32...
2024-06-23 17:04:01 INFO [auto_gptq.quantization.gptq] duration: 1.3305292129516602
2024-06-23 17:04:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.q_proj in layer 1/32...
2024-06-23 17:04:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 1/32...
2024-06-23 17:04:03 INFO [auto_gptq.quantization.gptq] duration: 1.3486604690551758
2024-06-23 17:04:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.o_proj in layer 1/32...
2024-06-23 17:04:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 1/32...
2024-06-23 17:04:07 INFO [auto_gptq.quantization.gptq] duration: 1.9133334159851074
2024-06-23 17:04:07 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.up_proj in layer 1/32...
2024-06-23 17:04:14 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 1/32...
2024-06-23 17:04:16 INFO [auto_gptq.quantization.gptq] duration: 2.168485641479492
2024-06-23 17:04:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.gate_proj in layer 1/32...
2024-06-23 17:04:16 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 1/32...
2024-06-23 17:04:17 INFO [auto_gptq.quantization.gptq] duration: 1.594611644744873
2024-06-23 17:04:17 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.down_proj in layer 1/32...
2024-06-23 17:04:51 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 1/32...
2024-06-23 17:05:05 INFO [auto_gptq.quantization.gptq] duration: 13.472443580627441
2024-06-23 17:05:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Start quantizing layer 2/32
2024-06-23 17:05:06 INFO [auto_gptq.modeling._base] Start quantizing layer 2/32
INFO - Quantizing self_attn.k_proj in layer 2/32...
2024-06-23 17:05:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 2/32...
2024-06-23 17:05:18 INFO [auto_gptq.quantization.gptq] duration: 1.8977351188659668
2024-06-23 17:05:18 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.v_proj in layer 2/32...
2024-06-23 17:05:18 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 2/32...
2024-06-23 17:05:19 INFO [auto_gptq.quantization.gptq] duration: 1.3267383575439453
2024-06-23 17:05:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.q_proj in layer 2/32...
2024-06-23 17:05:19 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 2/32...
2024-06-23 17:05:20 INFO [auto_gptq.quantization.gptq] duration: 1.349489450454712
2024-06-23 17:05:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.o_proj in layer 2/32...
2024-06-23 17:05:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 2/32...
2024-06-23 17:05:25 INFO [auto_gptq.quantization.gptq] duration: 1.9191405773162842
2024-06-23 17:05:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.up_proj in layer 2/32...
2024-06-23 17:05:31 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 2/32...
2024-06-23 17:05:33 INFO [auto_gptq.quantization.gptq] duration: 2.1688034534454346
2024-06-23 17:05:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.gate_proj in layer 2/32...
2024-06-23 17:05:33 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 2/32...
2024-06-23 17:05:35 INFO [auto_gptq.quantization.gptq] duration: 1.5943000316619873
2024-06-23 17:05:35 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.down_proj in layer 2/32...
2024-06-23 17:06:09 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 2/32...
2024-06-23 17:06:23 INFO [auto_gptq.quantization.gptq] duration: 13.496976375579834
2024-06-23 17:06:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Start quantizing layer 3/32
2024-06-23 17:06:24 INFO [auto_gptq.modeling._base] Start quantizing layer 3/32
INFO - Quantizing self_attn.k_proj in layer 3/32...
2024-06-23 17:06:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 3/32...
2024-06-23 17:06:35 INFO [auto_gptq.quantization.gptq] duration: 1.8941528797149658
2024-06-23 17:06:35 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.v_proj in layer 3/32...
2024-06-23 17:06:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 3/32...
2024-06-23 17:06:37 INFO [auto_gptq.quantization.gptq] duration: 1.3204479217529297
2024-06-23 17:06:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.q_proj in layer 3/32...
2024-06-23 17:06:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 3/32...
2024-06-23 17:06:38 INFO [auto_gptq.quantization.gptq] duration: 1.3427586555480957
2024-06-23 17:06:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.o_proj in layer 3/32...
2024-06-23 17:06:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 3/32...
2024-06-23 17:06:43 INFO [auto_gptq.quantization.gptq] duration: 1.9130713939666748
2024-06-23 17:06:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.up_proj in layer 3/32...
2024-06-23 17:06:49 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 3/32...
2024-06-23 17:06:51 INFO [auto_gptq.quantization.gptq] duration: 2.160127639770508
2024-06-23 17:06:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.gate_proj in layer 3/32...
2024-06-23 17:06:51 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 3/32...
2024-06-23 17:06:53 INFO [auto_gptq.quantization.gptq] duration: 1.5946838855743408
2024-06-23 17:06:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.down_proj in layer 3/32...
2024-06-23 17:07:27 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 3/32...
2024-06-23 17:07:40 INFO [auto_gptq.quantization.gptq] duration: 13.478856563568115
2024-06-23 17:07:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Start quantizing layer 4/32
2024-06-23 17:07:41 INFO [auto_gptq.modeling._base] Start quantizing layer 4/32
INFO - Quantizing self_attn.k_proj in layer 4/32...
2024-06-23 17:07:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 4/32...
2024-06-23 17:07:53 INFO [auto_gptq.quantization.gptq] duration: 1.8973746299743652
2024-06-23 17:07:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.v_proj in layer 4/32...
2024-06-23 17:07:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 4/32...
2024-06-23 17:07:54 INFO [auto_gptq.quantization.gptq] duration: 1.32810378074646
2024-06-23 17:07:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.q_proj in layer 4/32...
2024-06-23 17:07:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 4/32...
2024-06-23 17:07:56 INFO [auto_gptq.quantization.gptq] duration: 1.3488492965698242
2024-06-23 17:07:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.o_proj in layer 4/32...
2024-06-23 17:07:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 4/32...
2024-06-23 17:08:00 INFO [auto_gptq.quantization.gptq] duration: 1.9207582473754883
2024-06-23 17:08:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.up_proj in layer 4/32...
2024-06-23 17:08:07 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 4/32...
2024-06-23 17:08:09 INFO [auto_gptq.quantization.gptq] duration: 2.166910409927368
2024-06-23 17:08:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.gate_proj in layer 4/32...
2024-06-23 17:08:09 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 4/32...
2024-06-23 17:08:10 INFO [auto_gptq.quantization.gptq] duration: 1.602674961090088
2024-06-23 17:08:10 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.down_proj in layer 4/32...
2024-06-23 17:08:45 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 4/32...
2024-06-23 17:08:58 INFO [auto_gptq.quantization.gptq] duration: 13.484594583511353
2024-06-23 17:08:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Start quantizing layer 5/32
2024-06-23 17:08:59 INFO [auto_gptq.modeling._base] Start quantizing layer 5/32
INFO - Quantizing self_attn.k_proj in layer 5/32...
2024-06-23 17:09:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 5/32...
2024-06-23 17:09:11 INFO [auto_gptq.quantization.gptq] duration: 1.8931217193603516
2024-06-23 17:09:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.v_proj in layer 5/32...
2024-06-23 17:09:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 5/32...
2024-06-23 17:09:12 INFO [auto_gptq.quantization.gptq] duration: 1.3209638595581055
2024-06-23 17:09:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.q_proj in layer 5/32...
2024-06-23 17:09:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 5/32...
2024-06-23 17:09:13 INFO [auto_gptq.quantization.gptq] duration: 1.3436195850372314
2024-06-23 17:09:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.o_proj in layer 5/32...
2024-06-23 17:09:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 5/32...
2024-06-23 17:09:18 INFO [auto_gptq.quantization.gptq] duration: 1.9153153896331787
2024-06-23 17:09:18 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.up_proj in layer 5/32...
2024-06-23 17:09:24 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 5/32...
2024-06-23 17:09:26 INFO [auto_gptq.quantization.gptq] duration: 2.1616218090057373
2024-06-23 17:09:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.gate_proj in layer 5/32...
2024-06-23 17:09:26 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 5/32...
2024-06-23 17:09:28 INFO [auto_gptq.quantization.gptq] duration: 1.5955889225006104
2024-06-23 17:09:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.down_proj in layer 5/32...
2024-06-23 17:10:02 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 5/32...
2024-06-23 17:10:16 INFO [auto_gptq.quantization.gptq] duration: 13.4797523021698
2024-06-23 17:10:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Start quantizing layer 6/32
2024-06-23 17:10:17 INFO [auto_gptq.modeling._base] Start quantizing layer 6/32
INFO - Quantizing self_attn.k_proj in layer 6/32...
2024-06-23 17:10:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 6/32...
2024-06-23 17:10:28 INFO [auto_gptq.quantization.gptq] duration: 1.8976857662200928
2024-06-23 17:10:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.v_proj in layer 6/32...
2024-06-23 17:10:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 6/32...
2024-06-23 17:10:30 INFO [auto_gptq.quantization.gptq] duration: 1.3269011974334717
2024-06-23 17:10:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.q_proj in layer 6/32...
2024-06-23 17:10:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 6/32...
2024-06-23 17:10:31 INFO [auto_gptq.quantization.gptq] duration: 1.3436768054962158
2024-06-23 17:10:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.o_proj in layer 6/32...
2024-06-23 17:10:34 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 6/32...
2024-06-23 17:10:36 INFO [auto_gptq.quantization.gptq] duration: 1.9209647178649902
2024-06-23 17:10:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.up_proj in layer 6/32...
2024-06-23 17:10:42 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 6/32...
2024-06-23 17:10:44 INFO [auto_gptq.quantization.gptq] duration: 2.1706249713897705
2024-06-23 17:10:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.gate_proj in layer 6/32...
2024-06-23 17:10:44 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 6/32...
2024-06-23 17:10:46 INFO [auto_gptq.quantization.gptq] duration: 1.6023154258728027
2024-06-23 17:10:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.down_proj in layer 6/32...
2024-06-23 17:11:20 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 6/32...
2024-06-23 17:11:33 INFO [auto_gptq.quantization.gptq] duration: 13.492442607879639
2024-06-23 17:11:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Start quantizing layer 7/32
2024-06-23 17:11:35 INFO [auto_gptq.modeling._base] Start quantizing layer 7/32
INFO - Quantizing self_attn.k_proj in layer 7/32...
2024-06-23 17:11:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 7/32...
2024-06-23 17:11:46 INFO [auto_gptq.quantization.gptq] duration: 1.895052433013916
2024-06-23 17:11:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.v_proj in layer 7/32...
2024-06-23 17:11:46 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 7/32...
2024-06-23 17:11:47 INFO [auto_gptq.quantization.gptq] duration: 1.3228154182434082
2024-06-23 17:11:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.q_proj in layer 7/32...
2024-06-23 17:11:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 7/32...
2024-06-23 17:11:49 INFO [auto_gptq.quantization.gptq] duration: 1.3440525531768799
2024-06-23 17:11:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.o_proj in layer 7/32...
2024-06-23 17:11:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 7/32...
2024-06-23 17:11:53 INFO [auto_gptq.quantization.gptq] duration: 1.916480541229248
2024-06-23 17:11:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.up_proj in layer 7/32...
2024-06-23 17:12:00 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 7/32...
2024-06-23 17:12:02 INFO [auto_gptq.quantization.gptq] duration: 2.1629676818847656
2024-06-23 17:12:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.gate_proj in layer 7/32...
2024-06-23 17:12:02 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 7/32...
2024-06-23 17:12:03 INFO [auto_gptq.quantization.gptq] duration: 1.5948543548583984
2024-06-23 17:12:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.down_proj in layer 7/32...
2024-06-23 17:12:38 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 7/32...
2024-06-23 17:12:51 INFO [auto_gptq.quantization.gptq] duration: 13.477227926254272
2024-06-23 17:12:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Start quantizing layer 8/32
2024-06-23 17:12:52 INFO [auto_gptq.modeling._base] Start quantizing layer 8/32
INFO - Quantizing self_attn.k_proj in layer 8/32...
2024-06-23 17:13:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 8/32...
2024-06-23 17:13:04 INFO [auto_gptq.quantization.gptq] duration: 1.8969049453735352
2024-06-23 17:13:04 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.v_proj in layer 8/32...
2024-06-23 17:13:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 8/32...
2024-06-23 17:13:05 INFO [auto_gptq.quantization.gptq] duration: 1.3281970024108887
2024-06-23 17:13:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.q_proj in layer 8/32...
2024-06-23 17:13:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 8/32...
2024-06-23 17:13:06 INFO [auto_gptq.quantization.gptq] duration: 1.3449070453643799
2024-06-23 17:13:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.o_proj in layer 8/32...
2024-06-23 17:13:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 8/32...
2024-06-23 17:13:11 INFO [auto_gptq.quantization.gptq] duration: 1.920198917388916
2024-06-23 17:13:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.up_proj in layer 8/32...
2024-06-23 17:13:17 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 8/32...
2024-06-23 17:13:20 INFO [auto_gptq.quantization.gptq] duration: 2.16698956489563
2024-06-23 17:13:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.gate_proj in layer 8/32...
2024-06-23 17:13:20 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 8/32...
2024-06-23 17:13:21 INFO [auto_gptq.quantization.gptq] duration: 1.6000502109527588
2024-06-23 17:13:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.down_proj in layer 8/32...
2024-06-23 17:13:55 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 8/32...
2024-06-23 17:14:09 INFO [auto_gptq.quantization.gptq] duration: 13.490780591964722
2024-06-23 17:14:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Start quantizing layer 9/32
2024-06-23 17:14:10 INFO [auto_gptq.modeling._base] Start quantizing layer 9/32
INFO - Quantizing self_attn.k_proj in layer 9/32...
2024-06-23 17:14:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 9/32...
2024-06-23 17:14:21 INFO [auto_gptq.quantization.gptq] duration: 1.8957045078277588
2024-06-23 17:14:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.v_proj in layer 9/32...
2024-06-23 17:14:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 9/32...
2024-06-23 17:14:23 INFO [auto_gptq.quantization.gptq] duration: 1.3228318691253662
2024-06-23 17:14:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.q_proj in layer 9/32...
2024-06-23 17:14:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 9/32...
2024-06-23 17:14:24 INFO [auto_gptq.quantization.gptq] duration: 1.3435311317443848
2024-06-23 17:14:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing self_attn.o_proj in layer 9/32...
2024-06-23 17:14:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 9/32...
2024-06-23 17:14:29 INFO [auto_gptq.quantization.gptq] duration: 1.9151115417480469
2024-06-23 17:14:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.up_proj in layer 9/32...
2024-06-23 17:14:35 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 9/32...
2024-06-23 17:14:37 INFO [auto_gptq.quantization.gptq] duration: 2.1629490852355957
2024-06-23 17:14:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.gate_proj in layer 9/32...
2024-06-23 17:14:37 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 9/32...
2024-06-23 17:14:39 INFO [auto_gptq.quantization.gptq] duration: 1.5964548587799072
2024-06-23 17:14:39 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Quantizing mlp.down_proj in layer 9/32...
2024-06-23 17:15:13 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 9/32...
2024-06-23 17:15:26 INFO [auto_gptq.quantization.gptq] duration: 13.473958015441895
2024-06-23 17:15:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.0
INFO - Start quantizing layer 10/32
2024-06-23 17:15:28 INFO [auto_gptq.modeling._base] Start quantizing layer 10/32
